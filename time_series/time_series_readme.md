# Time Series

## Basics
### Time Series Definition
A time series is a chronological sequence of data obtained through systematic measurement or numerical observation at fixed time intervals of a variable of interest.

A time series is a realization of a sucession of observations of a random variable $\{Z(\tau): \tau \in T\}$ where $T\subseteq\mathbb{N}$ and takes on a set $S\subseteq\mathbb{R}$. This is, a time series is a sucession of observaions generated by a discrete-time continous stochastic process with parameter space $T$ and states space $S$. 

In particular, if the observations of a time series are taken on the moments $\tau_1, \tau_2, ..., \tau_n$ then the stochastic process will be denoted by ${Z(\tau_1), Z(\tau_2), ..., Z(\tau_n)}$ while the time series will be denoted by $\{Z_t : t\in T\}$ or just $\{Z_t\}$. Thus, when having $n$ succesive values of a time series, we will write $z_1, z_2,...,z_{n-1}, z_n$ in order to denote the observations taken in equidistant intervals $\tau_0 + h, \tau_0 + 2h, ..., \tau_0 + (n-1)h, \tau_0 + nh$ where $\tau_0$ is some point in time that works as origin and $h$ is the constant longitude of the interval of time that separates two adjacent observations. 



### Stationarity
Understanding and ensuring stationarity is crucial for accurate modeling, forecasting, and analysis in time series studies. In must of the cases, in order for a time series to be modeled, it must satisfy at least weak stationarity condition. Let $\{Z_t\}$ be a time series. 
  
Weak Sattionarity  
$\{Z_t\}$ is said to be stationary of second order or weakly sationaty if the mean, variance and the covariance of the process do not depend on time. This is:
* $E[Z_t] = \mu$ for some constant $\mu\in\mathbb{R}$ that do not depends on $t$.
* $Var[Z_t] = \sigma^2$ for some constant $\sigma^2\in\mathbb{R}^{+}$ that do not depends on $t$
* $Cov(Z_t, Z_{t+k}) = \gamma(k)$ where $\gamma(k)$ is a function of tha lag $k$ -callend the autovariance function- that do not depends on $t$.
  
Thus, neither the level of the series nor its variability depend on time. Regarding covariance, there is no dependence on time, but there is dependence on the lags between the variables. This leads to the idea that the series will exhibit the same general behavior regardless of when it is observed. That is, if a certain number of contiguous observations of a series were plotted, the resulting graph would be quite similar to that obtained by plotting the same number of contiguous observations, but $k$ periods forward or backward from the initially considered observations.
  
Strong  Stationarity  
$\{Z_t\}$ is said to be strictly or strongly stationay if the joint probability distribution of any set of observations is the same regardless of when they are observed. this is:
$$ f(Z_t, Z_{t+1}, \dots, Z_{t+m}) = f(Z_{t+k}, Z_{t+k+1}, \dots, Z_{t+k+m}) \quad \forall k, m \in \mathbb{N} $$

Note that strong stationariry implies weak sationarity. 
  
Strong stationarity vs. weak stationarity  
While strict stationarity requires that all aspects of the joint distribution remain unchanged over time, weak stationarity only requires the mean, variance, and autocovariance to be time-invariant, making it more applicable in practical scenarios.
  
Making a Series Stationary  
If a time series is non-stationary, it can often be transformed into a stationary series through:
* Differencing: Subtracting previous observations from current observations to stabilize variance. 
* Transformation: Applying mathematical transformations (e.g., logarithms or Box-Cox) to stabilize variance.
* Detrending: Removing trends or seasonality through various methods.
  
ADF Test  
Stationarity can be tested throug the Augmeted Dickey-Fuller Test (ADF Test). The ADF test has the following hypotheses:
* Null Hypothesis (H0): The time series has a unit root, meaning it is non-stationary.
* Alternative Hypothesis (H1): The time series does not have a unit root, meaning it is stationary.
  
You can do this through the followng code:  
`# Exectute Dickey-Fuller test for stationarity`  
`from statsmodels.tsa.stattools import adfuller`  
`ts = df['y']`  
`result = adfuller(ts)`  
`print('ADF Statistic:', result[0])`  
`print('p-value:', result[1])`  
`print('Critical Values:')`  
`for key, value in result[4].items():`  
`    print('\t%s: %.3f' % (key, value))`  
  
Reject the null hypothesis for suffiently small values of the $p$-value, for example $p<0.05$. This would mean that there is enough evidence suggest that the time series is stationary. If the $p$-value is not sufficiently small, for example, $p>0.5$, then there would not be enough evidence in order to suggest the time series is stationary. 



### Box - Jenkins Modeling Approach
The approach to time series analysis presented here follows the method suggested by Box and Jenkins in 1970. The fundamental idea of their approach lies in the strategy they propose for building models, which should not only be suitable for representing the behavior of observed data but should also be guided by the data themselves. This contrasts with the traditional approach that simply aims to achieve the best fit of preconceived models to the available data.

The model-building strategy consists of the following stages:
1. Model identification (within a general class of models and based on what the data suggest).
1. Estimation of the model's implicit parameters (using the most efficient technique available).
2. Verification of assumptions (to ensure that the results derived from the model can be considered valid).
3. Use of the model (for the purposes that motivated its construction).

In the model identification stage, it is necessary to define a class of models that is sufficiently general to practically represent any time series that may be observed in applications. This family of models is known as ARIMA models (AutoRegressive Integrated Moving Average).

The decomposition of time series assumes that the time series is composed of three main components: trend-cycle, which represents the long-term movement of the series; seasonality, which captures effects repeated annually with some consistency; and irregularity, which characterizes unpredictable and considered random movements. The methodology of time series decomposition aims to identify and estimate each of these components separately. Within this framework, it is considered that the trend-cycle and seasonality components constitute the deterministic or semi-deterministic part of the series, while the irregular component represents its non-deterministic or stochastic part. In this context, it is reasonable to infer that the population of interest consists of time series formed by the deterministic or semi-deterministic part combined with the possible realizations of the stochastic part. Nevertheless, we rather interpret a time series as a stochastic process. 




## ARMA models
The models for the time series that will be studied here are based on the idea that a time series, whose successive values can be highly dependent, can be considered to be generated from a series of independent random shocks. These random shocks are assumed to be independent realizations of a random variable with a constant mean (typically considered to be zero) and a fixed variance. In other words, a white noise porcess. Let $\{a_t : t\in\mathbb{N}\}$ be a succesion of random variables that, $(\forall  t\in\mathbb{N})$. A **white noise process** is such that:
1. $E(a_t) = \mu \in\mathbb{R}$  $(\forall  t\in\mathbb{N})$
2. $V(a_t) = \sigma_2 \in\mathbb{R}^{+}$ $(\forall  t\in\mathbb{N})$
3. $Cov(a_t, a_{t+1}) = 0$ $(\forall  t\in\mathbb{N})$
Then, we say  $\{a_t : t\in\mathbb{N}\}$ is a white noise process. We usually denote it just as $\{a_t\}$.
  
Let be the following:
* $\{Z_t\}$ time series. 
* $\{a_t\}$ white noise process. 
* $\mu\in\mathbb{R}$ the mean of the time series. 
* $\theta_1, \theta_2, ..., \theta_q \in\mathbb{R}$.
* $\phi_1, \phi_2, ..., \phi_p \in\mathbb{R}$.



### Operators
**Identity Operator**  
Let $ \{ Z_t \} $ be a time series. The identity operator $I$ is such that $IZ_t = Z_t$ $\forall t\in T$. 

**Backshift Operator**  
Let $\{Z_t\}$ be a time series. The backshift operator $B$ is such that $BZ_t = Z_{t-1}$ $\forall t\in T$. Moreover, for $t\in T$, we have that:
* $B^0Z_t = IZ_t = Z_t$
* $B^1Z_t = BZ_t = Z_{t-1}$
* $B^2Z_t = BBZ_t = BZ_{t-1}=BZ_{t-2}$  
.  
.  
.  
* $B^kZ_t = Z_{t-k}$ $(k<t)$

**Difference Operator**  
Let $\{Z_t\}$ be a time series. The differencing operator $\nabla$ is such that $\nabla Z_t = Z_{t} - Z_{t-1}$ $\forall t \in T$. Moreover, for $t\in T$, we have that:
* $\nabla Z_t = (1-B)Z_t$
* $\nabla^kZ_t = (1-B)^kZ_t = \sum_{j=0}^{k}\frac{k!}{j!(k-j)!}(-1)^jZ_{t-j}$ $(k \in \mathbb{N}\cup\{0\})$ 
Must note that a difference operator of order $k$ $\nabla^d$ automatically errases $d$ observations of the time series. 

**Backshift Polinomial**  
Let $\{Z_t\}$ be a time series, $g_0 = -1, g_1, g_2, ..., g_k \in\mathbb{R}$. A backshift polimonial $G(B)$ of order $k$ for $k \in \mathbb{N}\cup\{0\}$ is as follows. 
$$G(B) = 1 - g_1B - g_2B - g_2B^2 - ... - g_kB^k \\
= 1 - \sum_{j=1}^kg_jB^j \\
= -\sum_{j=0}^{k}g_jB^j  $$

**Racional Backhift Polinomial**  
Let $\{Z_t\}$ be a time series, $A(B), C(B)$ backshift polinomials such that $A(B) = -\sum_{j=0}^k a_jB^j$, $C(B) = -\sum_{j=0}^k c_jB^j$. A rational polinomial $G(B)$ for $k \in \mathbb{N}\cup\{0\}$ is as follows.
$$G(B) = \frac{A(B)}{C(B)} \\
= \frac{-\sum_{j=0}^k a_jB^j}{-\sum_{j=0}^k c_jB^j}$$ 
  
Note that the a backshift polinomial $G(B)$ could have an infinite number of coefficients, but a finite number of coefficients expressed as a rational backshift polinomial. For example, if $g\in\mathbb{R}$ is such that $|g|<1$, then $G(B) = 1 + gB + g^2B^2 + ... = \frac{1}{1-gB}$.



### ARIMA Models

**$AR(p)$**  
**Autoregressive Model with the last $p$ autoregressors**  
 The main idea behind an AR model is that the current value of a time series is regressed on its own previous values. Essentially, the future values of the series are predicted based on its $p$ past values.

$$(1-\phi_1B-\phi_2B^2-...-\phi_pB^p)(Z_t-\mu) = a_t $$
Or, in a compact manner:  
$$\phi(B)(Z_t-\mu) = a_t$$

**$MA(q)$**  
**Moving Average Model with a linear combinations of the last $q$ error terms**  
The main idea behind the MA model is that the the current value of a time series can be expressed as as a linear combination of past error terms (shocks or noise).

$$(Z_t - \mu) = (1-\theta_1B - \theta_2B^2-...-\theta_qB^q)a_t$$
Or, in a compact manner:  
$$(Z_t - \mu)  = \theta(B)a_t $$

  

**$ARMA(p,q)$**  
**Autoregrresive Moving Average Model**  
The Autoregressive Moving average model (ARMA) combines AR and MA models. This is, considers on both of the model main ideas:
* The current value of a time series is regressed on its own $p$ previous values
* The current value of a time series can be expressed as a linear combination of $q$ past error terms.

$$(1 - \phi_1 B - \phi_2 B^2 - \cdots - \phi_p B^p)(Z_t - \mu) = (1 - \theta_1 B - \theta_2 B^2 - \cdots - \theta_q B^q)a_t$$
Or, in a comact manner:
$$\phi(B)(Z_t-\mu) = \theta(B)a_t$$


**$ARIMA(p,d,q)$**    
**(Autorregresive Integrated Moving Averages with $(p,q)$ Coefficients and $d$ Differences)**  
Just and differnciated ARIMA in order for the AR part of the model to be stationary. 
$$(1-\phi_1B-\phi_2B^2-...-\phi_pB^p)\nabla^d(Z_t-\mu) = (1-\theta_1B - \theta_2B^2-...-\theta_qB^q)a_t $$
Or, in a compact manner:  
$$\phi(B)\nabla^d(Z_t-\mu) = \theta(B)a_t$$

  
**Yule General Model**  
This is a general expression for all of the above models given by the following:  
$$Z_t = \mu + a_t - \psi_1a_{t-1} - \psi_2a_{t-2}-... \\
      = \mu + \psi(B)a_t$$
Where $\psi(B) = 1 - \psi_1B - \psi_2B^2 - ...$ is a las polinomial that turns the process $\{a_t\}$ into the process $\{Z_t\}$. Thus, $\psi(B)$ acts like a linear filter. 

* For MA models, $\psi(B) = \theta(B)$.
* For AR models, $\psi(B) = \frac{1}{\phi(B)}$
* For ARMA and ARIMA models, $\psi(B) = \frac{\theta(B)}{\psi(B)}$
* Same for ARMA as for ARIMA models, but considering the $d$ diffrences.

#### Stationarity and Inveribility for ARIMA models
Consider a time series $\{Z_t\}$. In order for a AR model to be used, it must be sationary. In order for a MA model to be used, it must be invertible. In order fot an AMRA model to be used, it must be stationary and invertible. 

* An AR model will be **stationary** if the the roots of the backshift polynomial $\phi(B) = 1 - \phi_1B - \phi_2B^2 - .... - \phi_pB^p$ lie outside the unit circle in the complex plane.
* A MA model is said to be **invertible** if it can be expressed as an stationary AR process. Thus, the roots of the bckshift polynomial $1 + \theta_1B + \theta_2B^2+...+\theta_qB^q$ must lie outside the unit circle in the complex plane.
* An AR model  is always going to be invertible and a MA process is always going to be stationary. 
* An ARMA model will be stationary and invertible if the the roots of the backshift polynomials $\phi(B) = 1 - \phi_1B - \phi_2B^2 - .... - \phi_pB^p$ and  $1 + \theta_1B + \theta_2B^2+...+\theta_qB^q$  lie outside th unit circle in the complex plane.
* Same for ARMA as for ARIMA models, but considering the $d$ diffrences. 

Nothe that the model has to be estimated in order to ensure its stationarity and invertibility. 





## Others

#### ACF
**Autocovariance Function**
Let $\{Z_t\}$ be time series. The autocovariance function associated with $\{Z_t\}$ is given by:
$$\gamma_k E[(Z_t-\mu)(Z_{t+k}-\mu)] \quad (k \in\mathbb{Z})$$
Note that $\gamma_k = \gamma_{-k}$, 

**Autocorrelation Function**  
Let $\{Z_t\}$ be a time series. The autocorrelation function associated with $\{Z_t\}$ is given by:
$$\rho_k = \frac{E[(Z_t-\mu)(Z_{t+k}-\mu)]}{E[(Z_t-\mu)^2] } = \frac{\gamma_k}{\gamma_0}$$
Note that $\rho_k = \rho_{-k}$ and $\rho_0 = 1$. 

It's called autovariance or autocorrelation and not just covariance or correlation due to the fact that we're obtaining the correlation of the same variable, but lagged on time. 

We'll use autoccorelations to avoid the influence of squared measure units. The autocorrelation function, with $\mu$ and $V[a_t]$, is used in order to specify a stationary time series. 

In order to estimate the autoccorelation function through sample we must supose that $\{Z_t\}$ is an ergodic process. An stochastic process is said to be an erogodic if it's statistical properties can be deduced of a single big enough random sample. On the contrary, is said to be non ergodig if it changes erratically at inconsistent rates. Also, we must supose that $\mu<\infty$, $\gamma_k<\infty$ $\forall k \in \mathbb{Z}$, and $\displaystyle\lim_{n\to\infty}\frac{1}{n}\sum_{k=1}^n\rho_k = 0$. If so, we have the following estimator for a set with $n$ samples:
$$r_k = \frac{\sum_{t=1}^{n-k}(z_t-\bar{z})(z_{t+k}-\bar{z})}{\sum_{t=1}^n(z_t-\bar{z})}$$

Then, we may use an autocorrelation plot (ACP) given by the points $\{(k,r_k): k\in\{0,1,2,...,n\}\}$. Naturally, it'll be symetrical respect to $y$ axis and will have $(0,1)$ in it. A rapid decay to zero on the autocorrelation plot (ACP) indicates stationarity. 




## Extras
## Extras

-----
### Grid search (AIC)
he Akaike Information Criterion is a statistical measure used for model seletion. AIC is most useful when comparing models of the same data. It helps in comparing different models to determine which one best fits the data while accounting for model complexity. It is not as informative for absolute goodness-of-fit but is effective for relative comparisons. When you have several candidate models, calculate the AIC for each. The model with the lowest AIC is preferred. It is given by:

$$AIC = -2\ln(L) + 2k$$
Where:  
* $L$ is the log-likelihood function
* $k$ is the number of parameters

-----
