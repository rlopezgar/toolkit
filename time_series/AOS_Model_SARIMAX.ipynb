{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e841aa-fc32-48d0-a5ba-a468bb8ae5b2",
   "metadata": {},
   "source": [
    "# SRIMAX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d4983-1f6e-4576-b6e3-f31aebfa70fc",
   "metadata": {},
   "source": [
    "## Content\n",
    "* Elements\n",
    "* Data Preprocessing\n",
    "* Model Identification\n",
    "* Model Estimation\n",
    "* Model Verification\n",
    "* Model Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f778d8-20dd-4b4d-a2e4-e120c8a131dc",
   "metadata": {},
   "source": [
    "Import required tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1702ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import statsmodels as sm\n",
    "import sklearn\n",
    "import plotly\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0649b93",
   "metadata": {},
   "source": [
    "Get requiered config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c158d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f34b270-bdf3-4cc7-9a64-c2c599916399",
   "metadata": {},
   "source": [
    "## Elements\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d2b5b",
   "metadata": {},
   "source": [
    "### Definition\n",
    "**Endogenous Variables**  \n",
    "These are included in the model as the main time series data. The model aims to explain or predict these values based on historical data and other included factors.\n",
    "\n",
    "**Exogenous Variables**  \n",
    "These are included in the model as additional inputs that might affect the endogenous variable. \n",
    "\n",
    "### Sepecification\n",
    "**SARIMAX(p,d,q)(P,D,Q)_E**   \n",
    "**Seasonal Autoregressive Integrated Moving Averages Model with eXogenous Variables**  \n",
    "Let $\\{Z_t\\}$ be a time series with seasonal period $E$ that describes a fenomenom that might be influenced by the variables $\\{X_1\\}$, $\\{X_2\\}$, ..., $\\{X_n\\}$. A seaosnal autorgegressive inegrated moving averages model with exogenoius variables is expresed as follows:\n",
    "$$\\phi(B)\\Phi(B^{E})\\nabla^d\\nabla^D_E(Z_t) = \\theta(B)\\Theta(B^{E})a_t + \\sum_{i = 1}^{n} X_i$$\n",
    "Where:\n",
    "* $\\phi(B) = 1-\\phi_1B - \\phi_2B^{2} - ...- \\phi_qB^{q}$ is a backshift polynomial of order $q$ that represents the  autoregressive part of the model. Thus, the current value of a time series is regressed on its own $q$ previous values. This is, $Z_{t}, Z_{t-1}, ..., Z_{t-q}$.\n",
    "* $\\theta(B) = 1-\\theta_1B - \\theta_2B^{2} - ...- \\theta_pB^{p}$ is a backshift polynomial of order $p$ that represents the moving averages part of the  model. Thus, the current value of a time series can be expressed as a linear combination of $q$ past error terms. This is, the terms $a_{t}, a_{t-2}, ..., a_{t-p}$.\n",
    "* $\\Phi(B^E) = 1-\\Phi_1B^E - \\Phi_2B^{2E} - ...- \\Phi_QB^{QE}$ is a seasonal backshift polynomial of order $Q$ that represents the seasonal autoregressive part of the model. Thus, the current value of a time series is regressed on its own $Q$ previous seasonal values. This is, $Z_{t-E}, Z_{t-2E}, ..., Z_{t-QE}$.\n",
    "* $\\Theta(B^E) = 1-\\Theta_1B^E - \\Theta_2B^{2E} - ...- \\Theta_PB^{PE}$ is a seasonal backshift polynomial of order $P$ that represents the seasonal moving averages part of the  model. Thus, the current value of a time series can be expressed as a linear combination of $Q$ past seaosnal error terms. This is, the terms $a_{t-E}, a_{t-2E}, ..., a_{t-PE}$.\n",
    "* \\nabla_{E}^{D} is an seaosnal difference operator of order $D$.\n",
    "* $a_t$ is a white noise porcess.\n",
    "\n",
    "The main process $\\{Z_t\\}$ is referred as de endogenous variable while the processes $X_1, X_2,...,X_n$ are referred as exogenous variables or covariates. The idea behind the SARIMAX model is that these last variables may improve the performance of the model.\n",
    " \n",
    " Some examples of exogenous variables may include the following:\n",
    " * Macroeconomical variables\n",
    "    * Interest Rates\n",
    "    * Inflation Rates\n",
    "    * Central Bank Reserves\n",
    "    * GDP\n",
    "    * Unemployment Rates\n",
    "    * Exchange Rates\n",
    "    * Consumer Confidence Index\n",
    "    * Income Levels\n",
    "    * Population Growth\n",
    "    * Urbanization Trends\n",
    "* Financial and Business Metrics\n",
    "    * Stock Prices\n",
    "    * Corporate Earnings\n",
    "    * Operational Costs\n",
    "* Market-Specific Factors\n",
    "    * Price of Product\n",
    "    * Product Availability\n",
    "    * Competitor Pricing and Availability\n",
    "    * Product Market Share\n",
    "    * Marketing and Promotions\n",
    "    * Consumer Trends and Preferences\n",
    "    * Supply Chain Metrics\n",
    "    * Supply and Demand Metrics\n",
    "* External Events\n",
    "    * Regulatory Changes\n",
    "    * Geopolitical Events\n",
    "    * Natural Disasters\n",
    "* Commodity Prices\n",
    "    * Energy Commodities\n",
    "    * Metal Commodities\n",
    "    * Agricultural Commodities\n",
    "    * Livestock and Animal Products\n",
    "    * Soft Commodities\n",
    "\n",
    "\n",
    "### Forecastisting with Exogenous Variables\n",
    "Let \\{Z_t\\} be a time series with seasonal period $E$ that describes a fenomenom that might be influenced by the variables $\\{X_1\\}$, $\\{X_2\\}$, ..., $\\{X_n\\}$. Supose you've got $N$ data points for the variable endogenous variable $Z_t$ and your forecast horizon is $K$ periods forward. Thus, in order to make such forecast, the exogenous variables $X_1,X_2,...,X_n$ must have $N+K$ data points. Therefore, the exogenous variables must be proyected $K$ periods forward. \n",
    "\n",
    "Moreover, take the following considerations:\n",
    "* The exogenous variables, as well as the endogenous variable, must be mean-stabilized and variance-stabilized in order for them to be stationary. \n",
    "\n",
    "## Method\n",
    "In order to get the job done, we'll proceed with the Box-Jenkins approach for time series models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72526e4-2ddf-4475-aadd-6531b3f16bc7",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364c9f1",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc55fe9d-3e10-4451-b49c-ddfa41e47616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "loading_path = r'C:\\Users\\MXKRDL01\\OneDrive - Kellogg Company\\Desktop\\Workspace\\AOS\\AOS-Forecast\\Data\\BA_IZTAPALAPA_NORTE.csv'\n",
    "df = pd.read_csv(loading_path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb92b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "df = df[['ds', 'y', 'sales', \n",
    "        'inflation', 'consumer_confidence',  \n",
    "        'ba_general_sold_units', 'ba_general_sales_value',\n",
    "        'modern_channel_volume_share', 'modern_channel_avg_price_per_kilo']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ed796-c2ad-49ba-ba45-72c7092d4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure dates are fine\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547dac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for zeros\n",
    "indices_to_erase = df[df['y'] == 0].index\n",
    "indices_to_erase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a86a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erase zeros\n",
    "df = df.drop(indices_to_erase)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d946548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Make a dataset till the last data point of endogenous variable.\n",
    "df_present = df[df['ds'] <= '2024-07-01']\n",
    "\n",
    "# Make a dataset that includes the proyected values for exogenous variables\n",
    "df_furute = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4649529-c7df-4fad-bf4b-2db03c5f2d9e",
   "metadata": {},
   "source": [
    "### Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be04cfc-a60b-4cd2-8a40-cec433ce28fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View endogenous variable\n",
    "df_present.plot(x = 'ds', y = 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e628602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## View endogenous variable with exogenous variables\n",
    "\n",
    "# Import required tools\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Create an auxiliary data frame\n",
    "df_norm = df_present.copy()\n",
    "\n",
    "# Normalize in order to avoid scale issues\n",
    "df_norm = df_norm.drop('ds', axis=1)\n",
    "scaler =  RobustScaler()\n",
    "df_norm = scaler.fit_transform(df_norm)\n",
    "\n",
    "# Put the data back into a pandas df\n",
    "df_norm = pd.DataFrame(df_norm, columns=df.columns.drop('ds'))\n",
    "df_norm['ds'] = df_present['ds']\n",
    "\n",
    "# Plot\n",
    "df_norm.plot(x='ds', y=df.columns.drop('ds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab3dbbd-4181-461d-bdfb-6abbe9c853c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some basic statistics of engogenous variable as wel as exogenous variables\n",
    "df_present.describe(include = [np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01349f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the endogenous variable\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(df_present['y'], model='additive', period = 52 ) \n",
    "result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eca219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ACF and PACF of the endogenous variable\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "\n",
    "plot_acf(df_present['y'], lags=50)  \n",
    "plt.title('ACF Plot')\n",
    "plt.show()\n",
    "\n",
    "plot_pacf(df_present['y'], lags=50)  \n",
    "plt.title('PACF Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd26df",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a861836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put endogenous variable into a pandas series object\n",
    "endog_data = pd.Series(df_present['y'])\n",
    "exog_data = df_present.drop(columns = ['y', 'ds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff16eee",
   "metadata": {},
   "source": [
    "#### Seasonality Determination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bebe28",
   "metadata": {},
   "source": [
    "**Seasonality determination by visual inspection**  \n",
    "The goal is to look for patterns or repetitive cycles that recur at regular intervals. Check if there are obvious seasonal fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "# View time series\n",
    "plt.figure(figsize=(14, 8), dpi=100)  # Set the plot size to 14x8 inches and resolution to 100 DPI\n",
    "plt.plot(df_present['ds'], df_present['y'])\n",
    "\n",
    "# Set x-axis major locator to years and minor locator to months\n",
    "plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Time Series Plot')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee24eb2",
   "metadata": {},
   "source": [
    "**Time Series Decomposition**  \n",
    "The decomposition of time series assumes that the time series is composed of three main components: trend-cycle, which represents the long-term movement of the series; seasonality, which captures effects repeated annually with some consistency; and irregularity, which characterizes unpredictable and considered random movements. We'll focus in the seasonal part. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b087ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Assuming df is your DataFrame and 'ds' is the date column, 'y' is the value column\n",
    "df_E = df_present.copy()\n",
    "df_E['ds'] = pd.to_datetime(df_E['ds'])\n",
    "df_E.set_index('ds', inplace=True)\n",
    "\n",
    "# Decompose the time series\n",
    "result = seasonal_decompose(df_E['y'], model='additive', period=12)  # Adjust period as needed\n",
    "\n",
    "plt.figure(figsize=(14, 8), dpi=100)\n",
    "plt.plot(result.seasonal)\n",
    "plt.title('Seasonality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4814300",
   "metadata": {},
   "source": [
    "**ACP**  \n",
    "Use the ACF to detect seasonal periods by identifying regular patterns or peaks. \n",
    "* In the ACF plot, significant peaks at regular intervals indicate the presence of seasonality. For example, if your data is monthly and you see peaks every 12 lags, this suggests a seasonal period of 12 months.\n",
    "* The pattern of peaks will repeat with the seasonal period. For instance, if the ACF has a peak at lag 12, 24, 36, etc., this implies a seasonal period of 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Assuming df is your DataFrame and 'ds' is the date column, 'y' is the value column\n",
    "df_E = df_present.copy()\n",
    "df_E['ds'] = pd.to_datetime(df_E['ds'])\n",
    "df_E.set_index('ds', inplace=True)\n",
    "\n",
    "# Plot ACF\n",
    "plt.figure(figsize=(14, 8), dpi=100)  # Set the plot size to 14x8 inches and resolution to 100 DPI\n",
    "plot_acf(df_E['y'], lags=72)  # Adjust lags as needed to capture the seasonality\n",
    "\n",
    "# Customize x-axis ticks\n",
    "plt.xticks(range(0, 73, 12)) \n",
    "\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890615e",
   "metadata": {},
   "source": [
    "#### Variance Stabilization for Endogneous Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c658fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Satabilize variance\n",
    "from scipy.stats import boxcox\n",
    "endog_data_bc, l = boxcox(endog_data)\n",
    "print('Optimal lambda: ', l)\n",
    "\n",
    "# Put endog_data_bc into a pandas series object\n",
    "endog_data_bc = pd.Series(endog_data_bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176b6d43",
   "metadata": {},
   "source": [
    "#### Mean Stabilization for Endogenous Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8637a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Satbilize mean\n",
    "endog_data_bc_diff = endog_data_bc.diff().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e9135",
   "metadata": {},
   "source": [
    "#### Dickey-Fuller Test for Endogenous Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a76602e-e161-4a23-9b41-a162f3380112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exectute Dickey-Fuller test for stationarity\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "result = adfuller(endog_data_bc_diff)\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b2ab1e",
   "metadata": {},
   "source": [
    "#### Normalizaton for Exogenous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efffd176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required tools\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Apply the StandardScaler to each column\n",
    "exog_data_norm = pd.DataFrame(scaler.fit_transform(exog_data), columns=exog_data.columns)\n",
    "\n",
    "# View the normalized data\n",
    "print(exog_data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bacd1a",
   "metadata": {},
   "source": [
    "#### Dickey-Fuller Test for Exogenous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58509d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required tools\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Exectute Dickey-Fuller test for stationarity for non-nomralized exogenous variables\n",
    "for exog_var in ['sales', 'inflation', 'consumer_confidence', 'ba_general_sold_units', 'ba_general_sales_value', 'modern_channel_volume_share', 'modern_channel_avg_price_per_kilo']:\n",
    "    result = adfuller(exog_data[exog_var])\n",
    "    print('exogenous variable: ', exog_var)\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ffca58-c0c1-49cf-a9c4-68b6025e7f7f",
   "metadata": {},
   "source": [
    "## Model Identification\n",
    "This section is about finding the $(p,d,q)(P,D,Q)_E$ parameters of a SARIMA model in order to get the best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa91bf8",
   "metadata": {},
   "source": [
    "### Grid search (AIC)\n",
    "he Akaike Information Criterion is a statistical measure used for model seletion. AIC is most useful when comparing models of the same data. It helps in comparing different models to determine which one best fits the data while accounting for model complexity. It is not as informative for absolute goodness-of-fit but is effective for relative comparisons. When you have several candidate models, calculate the AIC for each. The model with the lowest AIC is preferred. It is given by:\n",
    "\n",
    "$$AIC = -2\\ln(L) + 2k$$\n",
    "Where:  \n",
    "* $L$ is the log-likelihood function\n",
    "* $k$ is the number of parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698737f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4b9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "\n",
    "# Define the p, d, q, P, D, Q, s parameter ranges\n",
    "p = d = q = range(0, 3)\n",
    "P = D = Q = range(0, 3)\n",
    "s = [4,8,12,16,20,24]  # Seasonal periods to consider\n",
    "\n",
    "# Generate all different combinations of p, d, q, P, D, Q\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "seasonal_pdq = list(itertools.product(P, D, Q, s))\n",
    "\n",
    "# Perform grid search\n",
    "best_aic = float(\"inf\")\n",
    "best_params = None\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for param in pdq:\n",
    "        for param_seasonal in seasonal_pdq:\n",
    "            try:\n",
    "                model = SARIMAX(endog = endog_data_bc,\n",
    "                                exog = exog_data_norm,\n",
    "                                order=param,\n",
    "                                seasonal_order=(param_seasonal[0], param_seasonal[1], param_seasonal[2], param_seasonal[3]),\n",
    "                                )\n",
    "                results = model.fit()\n",
    "                if results.aic < best_aic and param != (0,0,0):\n",
    "                    best_aic = results.aic\n",
    "                    best_params = (param, param_seasonal)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "print(f'Best SARIMA model: order={best_params[0]}, seasonal_order={best_params[1]} with AIC={best_aic}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3b712",
   "metadata": {},
   "source": [
    "Thus, for some coefficients $\\phi_1, \\phi_2, \\Theta_1, \\Theta_2\\in\\mathbb{R}$ and the Box-Cox transformated time series $Z_t^{'}$,the SARIMAX model is:\n",
    "\n",
    "$$(1- \\phi_1B -\\phi_2B^2)\\nabla^1Z_t^{'} = (1-\\Theta_1B^6-\\Theta_2B^{12})a_t + CC_t + U_t + GS_t + GDP_t + IR_t + MXNUSD_t$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def29eff",
   "metadata": {},
   "source": [
    "## Model Estimation\n",
    "In order to estimate the model coefficients we're going to use `statsmodels.tsa.statespace.sarimax.SARIMAX`. Thus, in order to get these, for a time series with edogenous variable previously box-cox transformed `endog_data_bc` stored on a `Series` object and exogenous data previously nomralized `exog_data_norm` stored on a `DataFrame` object  and parameters `(p,d,q)` and `(P,D,Q,s)` use the following code:\n",
    "  \n",
    "`from statsmodels.tsa.statespace.sarimax import SARIMAX`  \n",
    "`model = SARIMAX(endog = endog_data,`  \n",
    "`                 exog = exog_data,`  \n",
    "`                 order = (p,d,q),`  \n",
    "`                 seasonal_order = (P,D,Q,s))`  \n",
    "`model = model.fit()`  \n",
    "`model.summary()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3730ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Best parameter combination\n",
    "best_pdq = (2,0,1)\n",
    "best_PDQs = (2,2,0,24)\n",
    "\n",
    "# Estimate model\n",
    "model = SARIMAX(endog = endog_data_bc,\n",
    "                exog = exog_data_norm,\n",
    "                order = best_pdq,\n",
    "                seasonal_order = best_PDQs)\n",
    "model = model.fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f0b60",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "In order to evaluate the model we're going to use two approaches. \n",
    "* Practical approach: we're interested on getting a nice fitted model with forecasting power. Thus, we're just going to check in sample metrics and forward cross-validation metrics\n",
    "* Formal approach: We want to validate the assumptions on which the model is built. Thus, we'll validate each assumption of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eb32b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9aba0c7",
   "metadata": {},
   "source": [
    "### In Sample Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_absolute_error\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# Best parameter combination\n",
    "best_pdq = (2,0,1)\n",
    "best_PDQs = (2,2,0,24)\n",
    "\n",
    "# Get in-sample fitted values\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    best_model = SARIMAX(endog = endog_data_bc,\n",
    "                         exog = exog_data_norm, \n",
    "                         order = best_pdq,\n",
    "                         seasonal_order = best_PDQs)\n",
    "    best_results = best_model.fit()\n",
    "    fitted_values = best_results.fittedvalues\n",
    "\n",
    "# Check for negative values and replace them with a small positive value. This is required for the Box-Cox transformation.\n",
    "# We'll save the index and make it 1e-6 after the transformation.\n",
    "negative_index = np.where(fitted_values < 0)[0]\n",
    "fitted_values = np.where(fitted_values < 0, 1e-6, fitted_values)\n",
    "\n",
    "# Inverse Box-Cox transformation\n",
    "def inverse_boxcox(y_transformed, lmbda):\n",
    "    if lmbda == 0:\n",
    "        return np.exp(y_transformed)\n",
    "    else:\n",
    "        return np.exp(np.log(lmbda * y_transformed + 1) / lmbda)\n",
    "\n",
    "# Apply inverse Box-Cox transformation to fitted values\n",
    "ts, l = boxcox(df_present['y'])\n",
    "fitted_values = inverse_boxcox(fitted_values, l)\n",
    "actual_values = df_present['y']\n",
    "\n",
    "# Substitute negative values that were converted to 1e-6 and Box-Cox inverse-transformed for 1e-6.\n",
    "fitted_values[negative_index] = 1e-6\n",
    "\n",
    "# Calculate the performance metrics\n",
    "r2 = r2_score(actual_values, fitted_values)\n",
    "mape = mean_absolute_percentage_error(actual_values, fitted_values)\n",
    "mae = mean_absolute_error(actual_values, fitted_values)\n",
    "\n",
    "print('r2: ', r2)\n",
    "print('mape: ', mape)\n",
    "print('mae: ', mae)\n",
    "\n",
    "# Create the plot using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add actual values trace\n",
    "fig.add_trace(go.Scatter(x = df_present['ds'], y = actual_values, mode='lines', name='Real'))\n",
    "\n",
    "# Add fitted values trace\n",
    "fig.add_trace(go.Scatter(x = df_present['ds'], y = fitted_values, mode='lines', name='Fitted', line=dict(color='red')))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Real vs. Fitted Values',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Values',\n",
    "    legend=dict(x=0, y=1)\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary fix #\n",
    "df_re = pd.DataFrame({'ds': df_present['ds'], 'Real': actual_values, 'Fitted': fitted_values})\n",
    "df_re = df_re[df_re['ds']>= '2023-06-01']\n",
    "\n",
    "actual_values = df_re['Real']\n",
    "fitted_values = df_re['Fitted']\n",
    "\n",
    "# Calculate the performance metrics\n",
    "r2 = r2_score(actual_values, fitted_values)\n",
    "mape = mean_absolute_percentage_error(actual_values, fitted_values)\n",
    "mae = mean_absolute_error(actual_values, fitted_values)\n",
    "\n",
    "print('r2: ', r2)\n",
    "print('mape: ', mape)\n",
    "print('mae: ', mae)\n",
    "\n",
    "# Create the plot using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add actual values trace\n",
    "fig.add_trace(go.Scatter(x = df_re['ds'], y = actual_values, mode='lines', name='Real'))\n",
    "\n",
    "# Add fitted values trace\n",
    "fig.add_trace(go.Scatter(x = df_re['ds'], y = fitted_values, mode='lines', name='Fitted', line=dict(color='red')))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Real vs. Fitted Values',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Values',\n",
    "    legend=dict(x=0, y=1)\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9c4d0",
   "metadata": {},
   "source": [
    "### Forward Cross-Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8143f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# Best parameter combination\n",
    "best_pdq = (2,0,1)\n",
    "best_PDQs = (2,2,0,24)\n",
    "\n",
    "# Define the number of folds and the number of periods to forecast\n",
    "k = 5\n",
    "n_periods = 10\n",
    "\n",
    "# Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=k, test_size=n_periods)\n",
    "\n",
    "# Placeholder for the performance metrics\n",
    "r2_scores = []\n",
    "oos_r2_scores = []\n",
    "mape_scores = []\n",
    "mae_scores = []\n",
    "iteration = 0\n",
    "\n",
    "# Cross-validation\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for train_index, test_index in tscv.split(endog_data_bc):\n",
    "        train_endog, test_endog = endog_data_bc.iloc[train_index], endog_data_bc.iloc[test_index]\n",
    "        train_exog, test_exog = exog_data_norm.iloc[train_index], exog_data_norm.iloc[test_index]\n",
    "        \n",
    "        # Fit the SARIMA model\n",
    "        model = SARIMAX(endog = train_endog, \n",
    "                        exog = train_exog,\n",
    "                        order = best_pdq,\n",
    "                        seasonal_order = best_PDQs)\n",
    "        results = model.fit(disp=True)\n",
    "        \n",
    "        # Forecast\n",
    "        forecast = results.get_forecast(steps = n_periods, exog = test_exog)\n",
    "        forecast_values = forecast.predicted_mean\n",
    "\n",
    "        # Inverse Box-Cox transformation\n",
    "        def inverse_boxcox(y_transformed, lmbda):\n",
    "            if lmbda == 0:\n",
    "                return np.exp(y_transformed)\n",
    "            else:\n",
    "                return np.exp(np.log(lmbda * y_transformed + 1) / lmbda)\n",
    "        \n",
    "        # Apply inverse Box-Cox transformation to fitted values\n",
    "        ts, l = boxcox(df_present['y'])\n",
    "        forecasted_values = inverse_boxcox(forecast_values, l)\n",
    "        actual_values = inverse_boxcox(test_endog, l)\n",
    "        \n",
    "        # Comparison between actual values and forecast\n",
    "        df_comparison = pd.DataFrame({'y': actual_values.values.reshape(n_periods,), 'yhat': forecasted_values.values})\n",
    "        \n",
    "        # Calculate the performance metrics\n",
    "        r2 = r2_score(actual_values, forecasted_values)\n",
    "        def oos_r2_score(y_true, y_pred, y_dummy_pred):\n",
    "            mse_pred = mean_squared_error(y_true, y_pred)\n",
    "            mse_dummy = mean_squared_error(y_true, y_dummy_pred)\n",
    "            oos_r2 = 1 - (mse_pred / mse_dummy)\n",
    "            return oos_r2\n",
    "        oos_r2 = oos_r2_score(actual_values, forecasted_values, np.full(len(actual_values), np.mean(train_endog)))\n",
    "        mape = mean_absolute_percentage_error(actual_values, forecasted_values)\n",
    "        mae = mean_absolute_error(actual_values, forecasted_values)\n",
    "\n",
    "        # Print the performance metrics\n",
    "        print('iteration: ', iteration)\n",
    "        print('train size:', len(train_endog))\n",
    "        print('test size: ', n_periods)\n",
    "        print('r2: ', r2)\n",
    "        print('oos_r2: ', oos_r2)\n",
    "        print('mape: ', mape)\n",
    "        print('mae: ', mae)\n",
    "        print(df_comparison)\n",
    "\n",
    "        r2_scores.append(r2)\n",
    "        oos_r2_scores.append(oos_r2)\n",
    "        mape_scores.append(mape)\n",
    "        mae_scores.append(mae)\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "# Average R² and MAPE across all folds\n",
    "average_r2 = np.mean(r2_scores)\n",
    "average_oos_r2 = np.mean(oos_r2_scores)\n",
    "average_mape = np.mean(mape_scores)\n",
    "average_mae = np.mean(mae_scores)\n",
    "\n",
    "print('GENERAL PERFORMANCE METRICS')\n",
    "print('average r2: ', average_r2)\n",
    "print('average oos_r2: ', average_oos_r2)\n",
    "print('average mape: ', average_mape)\n",
    "print('average mae: ', average_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b6c34",
   "metadata": {},
   "source": [
    "### Assumptions Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f1f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0df0970",
   "metadata": {},
   "source": [
    "## Model Use\n",
    "Now let's predict some future periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16949588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set forecasting horizon\n",
    "n_periods = 10\n",
    "\n",
    "# Set exogenous variables proyected data\n",
    "exog_data_future = df_furute.drop(columns = ['y', 'ds']).tail(n_periods)\n",
    "\n",
    "# Normalice exogenpus vrariables proyecytion\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "exog_data_future_norm = pd.DataFrame(scaler.fit_transform(exog_data_future), columns=exog_data_future.columns)\n",
    "\n",
    "# Best parameter combination\n",
    "best_pdq = (2,1,0)\n",
    "best_PDQs = (0,0,2,6)\n",
    "\n",
    "# Fit the model with the best parameters\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    best_model = SARIMAX(endog = endog_data_bc,\n",
    "                         exog = exog_data_norm, \n",
    "                         order = best_pdq, \n",
    "                         seasonal_order = best_PDQs)\n",
    "    best_results = best_model.fit()\n",
    "  \n",
    "# Make forecast\n",
    "forecast = best_results.get_forecast(steps = n_periods, exog = exog_data_future_norm)\n",
    "\n",
    "# Extract predicted mean and confidence intervals\n",
    "predicted_mean = forecast.predicted_mean\n",
    "confidence_intervals = forecast.conf_int()  \n",
    "  \n",
    "# Combine into a single DataFrame\n",
    "forecast_df = pd.DataFrame({\n",
    "    'predicted_mean': predicted_mean,\n",
    "    'lower_bound': confidence_intervals.iloc[:, 0],\n",
    "    'upper_bound': confidence_intervals.iloc[:, 1]  \n",
    "})  \n",
    "  \n",
    "# Inverse Box-Cox transformation`\n",
    "def inverse_boxcox(y_transformed, lmbda):   \n",
    "    if lmbda == 0:\n",
    "        return np.exp(y_transformed)\n",
    "    else:\n",
    "        return np.exp(np.log(lmbda * y_transformed + 1) / lmbda)\n",
    "ts, l = boxcox(df_present['y'])\n",
    "\n",
    "# Apply inverse Box-Cox transformation to the forecast DataFrame  \n",
    "forecast_df = forecast_df.map(lambda x: inverse_boxcox(x, l))  \n",
    "\n",
    "forecast_df.head(n_periods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d56642",
   "metadata": {},
   "source": [
    "## Pendientes\n",
    "* Estacionariedad e invertibilidad estacional\n",
    "* Ahondar en tranformación potencia Box-Cox\n",
    "* Ahondar en número óptimo de diferencias estacionales y no estacionales\n",
    "* Evaluar supuestos del modelo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
