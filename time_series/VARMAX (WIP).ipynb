{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e841aa-fc32-48d0-a5ba-a468bb8ae5b2",
   "metadata": {},
   "source": [
    "# VARMAX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d4983-1f6e-4576-b6e3-f31aebfa70fc",
   "metadata": {},
   "source": [
    "## Content\n",
    "* Elements\n",
    "* Data Preprocessing\n",
    "* Model Identification\n",
    "* Model Estimation\n",
    "* Model Verification\n",
    "* Model Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f778d8-20dd-4b4d-a2e4-e120c8a131dc",
   "metadata": {},
   "source": [
    "Import required tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1702ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time \n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import statsmodels as sm\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb5023f",
   "metadata": {},
   "source": [
    "Get required config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e91d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dbede1-7149-43b3-af34-a00f7ef0ffb4",
   "metadata": {},
   "source": [
    "## Elements\n",
    "### Definition\n",
    "VARMAX (Vector Autoregressive Moving Average with Exogenous Variables) isa linear model for multivariate stationary time series. This is, it models data that involves both autoregressive behavior and moving average components across multiple interrelated time series. Moreover, it can also include exogenous variables for capturing the influece of external variables over the time series. \n",
    "  \n",
    "The following are the components of the VARMAX model:\n",
    "   \n",
    "**Autoregressive Component ($AR(p)$)**  \n",
    "The main idea behind an AR part of the model is that the current value of a time series is regressed on its own previous values. Essentially, the future values of the series are predicted based on its $p$ past values. \n",
    "  \n",
    "**Moving Average Component ($MA(q)$)**  \n",
    "The main idea behind the MA part of the model is that the the current value of a time series can be expressed as as a linear combination of past $q$ error terms (shocks or noise).  \n",
    "  \n",
    "**Exogenous Variables Component ($BX$)**  \n",
    "This component of the model consider the influence of external variables over the different endogenous time series over time.\n",
    "  \n",
    "Thus, the VARMAX model captures the  internal relationship between the endogenous time series and the influence of external factors. \n",
    "  \n",
    "---\n",
    "  \n",
    "### Specification\n",
    "Let be the following:  \n",
    "* $\\{y_t\\}_1,...,\\{y_t\\}_n$ time series. Asume we have $m$ occurrences for each of the $n$ time series whose data is chronologically arranged on a matrix $Y\\in\\mathcal{M}_{m\\times n}^{(\\mathbb{R})}$ so $Y_t = ren(Y)_t\\in\\mathbb{R}^n$ is the value of the time series at time $t$. \n",
    "* $ x_1,..., x_{n_{exog}}$ external variables. Asume we have $m$ occurences for each of the $n_{exog}$ external variables whose data is chronologically arranged on a matrix $X\\in\\mathbb{M}_{m\\times n_{exog}}^{(\\mathbb{R})}$ so $X_t = ren(X)_t\\in\\mathbb{R}^n$ is the value of the external variables at time $t$.\n",
    "* $\\{\\varepsilon_t\\}$ a white noise process whose realizaton at time $t$ is such that $a_t\\in\\mathbb{R}^n$.\n",
    "* $\\mu\\in\\mathbb{R}^n$ the intercepts. They might be set to zero. \n",
    "* $p$ the order of the autoregressive component of the model. \n",
    "* $q$ the order of the moving average component of the model. \n",
    "* $\\Phi_i\\in\\mathcal{M}_{n}^{(\\mathbb{R})}$ for $1\\leq i \\leq p$ the coefficient matrices for the autoregressive model component at lag $i$.\n",
    "* $\\Theta_j\\in\\mathcal{M}_{n}^{(\\mathbb{R})}$ for $1\\leq j \\leq q$ the coefficient matrices for the moving averages model component at lag $j$.\n",
    "* $B\\in\\mathcal{M}_{n\\times n_{exog}}^{(\\mathbb{R})}$ the coefficient matrix for the exogenous variables. \n",
    "  \n",
    "The VARMAX model is represented as follows:\n",
    "$$Y_t = \\mu + \\displaystyle\\sum_{i=1}^p\\Phi_i Y_{t-i} + \\displaystyle\\sum_{j=1}^q \\Theta_j \\varepsilon_{t-j} + BX_t + \\varepsilon_t$$\n",
    "  \n",
    "Where:\n",
    "* $\\displaystyle\\sum_{i=1}^p\\Phi_i Y_{t-i}$ is the autoregressive component of the model where:\n",
    "    * $Y_k\\in\\mathbb{R}^n$ are the time series values at time $k$.\n",
    "    * $\\Phi_i\\in\\mathcal{M}_{n}(\\mathbb{R})$ are the AR coeffiecients. \n",
    "* $\\displaystyle\\sum_{j=1}^q \\Theta_j \\varepsilon_{t-j}$ is the moving averages components of the model where:\n",
    "    * $\\varepsilon_k\\in\\mathbb{R}^n$ are the noise terms values at time $k$.\n",
    "    * $\\Theta_j\\in\\mathcal{M}_{n}(\\mathbb{R})$ are the MA coeffiecients. \n",
    "* $BX_t$ is the exogenous variable component where:\n",
    "    * $X_t\\in\\mathbb{R}^{n_{exog}}$ are the exogenous variables values at time $t$.\n",
    "    * $B\\in\\mathcal{M}_{n\\times n_{exog}}(\\mathbb{R})$ are the exogenous variable coefficients.\n",
    "* $\\mu\\in\\mathbb{R}^n$ is the intercept term.\n",
    "* $\\varepsilon_t\\mathbb{R}^n$ is the noise term at time $t$. \n",
    "\n",
    "Alternativeley, the VARMAX model be expresed as follows:\n",
    "$$\\Phi(L)y_t = \\mu + \\Theta(L)\\epsilon_t + BX_t$$\n",
    "  \n",
    "Where:\n",
    "* $\\Phi(L) = I_n -\\Phi_1L - \\Phi_2L^2 - ... - \\Phi_pL^p$ are lag polynomial matrices corresponding to the AR part with $\\Phi_1, \\Phi_2, ..., \\Phi_p \\in \\mathcal{M}_n(\\mathbb{R})$.\n",
    "* $\\Theta(L) = I_n - \\Theta_1L - \\Theta_2L^2 - ... - \\Theta_qL^q$ are lag polynomial matrices corresponding to the MA part with $\\Theta_1, \\Theta_2, ..., \\Theta_q \\in \\mathcal{M}_n(\\mathbb{R})$.\n",
    "* $BX_t$ is the exogenous component with $X_t\\in\\mathbb{R}^{n_{exog}}$ and $B\\in\\mathcal{M}_{n\\times n_{exog}}(\\mathbb{R})$\n",
    "* $\\mu\\in\\mathbb{R}^n$ is the intercept term.\n",
    "  \n",
    "---\n",
    "  \n",
    "### Parameter Estimation\n",
    "Once $p,q$ are defined, $pn^2 + qn^2 + nn_{exog} + n + \\frac{n(n+1)}{2}$ parameters must be estimated in order for the model to be used.\n",
    "* $p$ matrices $\\Phi_i\\in\\mathcal{M}_n(\\mathbb{R})$ for the VAR component. Thus, $p\\cdot n^2$ parameters must be estimated for this part. \n",
    "* $q$ matrrices $\\Theta_j\\in\\mathcal{M}_n(\\mathbb{R})$ $qn^2$ for the VMA component. Thus, $q\\cdot n^2$ parameters must be estimated. \n",
    "* One matrix $B\\in\\mathcal{M}_{n\\times n_{exog}}^{(\\mathbb{R})}$ for the exogenous variable component. Thus, $n\\cdot n_{exog}$ parameters must be estimated. \n",
    "* One vector $\\mu\\in\\mathbb{R}^n$ for the intercpets. Thus, $n$ parameters must be estimated. \n",
    "* One symetrical matrix $\\Sigma\\in\\mathcal{M}_n(\\mathbb{R})$ for covariance error matrix. Thus, $\\frac{n(n+1)}{2}$ must be estimated.\n",
    "\n",
    "The Maximum Likelihood Estimation (MLE) method is used to estimate the parameters of a VARMAX model by maximizing the log-likelihood function, which is based on the observed data and the assumed multivariate normal distribution of the error terms. The main parameters estimated include the autoregressive coefficients, moving average coefficients, exogenous variable coefficients, error covariance matrix, and intercepts. These estimates are obtained through iterative optimization techniques that maximize the likelihood of the observed data under the model.\n",
    "  \n",
    "---\n",
    "  \n",
    "### Assumptions\n",
    "1. **Linearity**  \n",
    "The model is a linear function of past values and errors. This is:\n",
    "$$Y_t = \\mu + \\sum_{i=1}^p \\Phi_i Y_{t - i} + \\sum_{j=1}^q \\Theta_j \\varepsilon_{t - j} + BX_t + \\varepsilon_t$$\n",
    "  \n",
    "2. **Stationarity of AR process**  \n",
    "The AR part of the model. Consider the lag polynomial $\\Phi(z) = I_n - \\Phi_1z - \\Phi_2z^2 - ... - \\Phi_pz^p$ for $z\\in\\mathbb{C}$ and examine the roots of the determinant equation $det(\\Phi(z)) = 0$. All of them must lie outside the unit circle in the complex plane. This is, $|z| > 1$ for $z$ such that $det(\\Phi(z)) = 0$.\n",
    "  \n",
    "3. **Invertibility of MA process**   \n",
    "The MA part of the model is invertible. Consider the lag polynomial $\\Theta(z) = I_n + \\Theta_1z + \\Theta_2z^2 + ... + \\Theta_qz^q$ for $z\\in\\mathbb{C}$ and examine the roots of the determinant equation $det(\\Theta(z)) = 0$. All of them must lie outside the unit circle in the complex plane. This is, $|z| > 1$ for $z$ such that $det(\\Theta(z)) = 0$.\n",
    "\n",
    "4. **Exogeneity of $X_t$**  \n",
    "The exogenous variables are strictly exogenousm, meaning $X_t$ is uncorrelated with past, present, and future errors. This is:\n",
    "\n",
    "$$\\mathbb{E}[\\varepsilon_t \\mid X_s] = 0, \\quad \\forall t,s$$\n",
    "\n",
    "5. **Error Term Properties**  \n",
    "The process $\\{\\varepsilon_t\\}$ is a white noise process. This is:\n",
    "* It has zero mean or $\\mathbb{E}[\\varepsilon_t] = \\mathbf{0}_n$.\n",
    "* Homoskedasticity or $\\mathbb{E}[\\varepsilon_t \\varepsilon_t^\\top] = \\Sigma$ where $\\Sigma$ is positive definite and constant covariance matrix.\n",
    "* No autocorrelation or $\\mathbb{E}[\\varepsilon_t \\varepsilon_s^\\top] = \\mathbf{0}_{n \\times n} \\quad \\forall t \\neq s$.\n",
    "\n",
    "6. **No Perfect Multicollinearity**  \n",
    "The regressors formed by lagged time series values $Y_t$, lagged errors $\\varepsilon_t$, and lagged exogenous variables $X_t$ should not be perfectly collinear.\n",
    "\n",
    "7. **Finite Moments**  \n",
    "The process $\\{Y_t\\}$ and $\\{\\varepsilon_t\\}$ have finite second moments. This is:\n",
    "\n",
    "$$\\mathbb{E}[\\|\\mathbf{y}_t\\|^2] < \\infty, \\quad \\mathbb{E}[\\|\\varepsilon_t\\|^2] < \\infty $$\n",
    "  \n",
    "---\n",
    "  \n",
    "### Other considerations\n",
    "**Seasonality**    \n",
    "VARMAX model can also be provided with seasonality. \n",
    "\n",
    "\n",
    "**Impulse Response Analysis**  \n",
    "Impulse Response Function (IRF): One of the useful features of the VARMAX model is that it allows for impulse response analysis. This analysis helps in understanding how a shock or sudden change in one variable (e.g., a marketing campaign or a sudden price change) can affect the entire system of time series over time.\n",
    "Use case: For example, if a company launches a new product, VARMAX can be used to analyze how the product's launch (an exogenous shock) affects sales, inventory, and marketing costs over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4d38d",
   "metadata": {},
   "source": [
    "## Example with Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c9e5af",
   "metadata": {},
   "source": [
    "### Required Data\n",
    "The dataset contains 36 months of monthly retail sales data for two products, labeled Sales A and Sales B, spanning from January 2018 to December 2020.\n",
    "\n",
    "* Endogenous Variables\n",
    "Sales A and Sales B are the two endogenous time series representing monthly sales volumes for two distinct product categories or stores.\n",
    "    * Both series exhibit an overall increasing trend over the three-year period, reflecting growth in sales.\n",
    "    * Their sales patterns also show fluctuations due to seasonal and promotional effects, as well as some random noise to simulate real-world variability.\n",
    "\n",
    "* Exogenpus Variables\n",
    "There are two exogenous variables included, which influence the sales. \n",
    "    * Promo Index is a numeric variable representing the intensity of promotional activities or marketing campaigns during each month.This variable follows a smooth cyclical pattern with added noise, simulating promotional cycles such as quarterly or seasonal sales campaigns. Higher promo index values generally increase sales for both products.\n",
    "    * Holiday Flag is a binary indicator variable marking whether a given month includes a major holiday shopping season (specifically November and December).These months typically experience significant boosts in sales due to holidays and year-end shopping events. When the flag is 1 (holiday months), sales tend to increase markedly for both products.\n",
    "\n",
    "The dataset is indexed by month start dates (YYYY-MM-DD) for clear time ordering and easy time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970f7b3a",
   "metadata": {},
   "source": [
    "**Generate Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3233ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate monthly data for three months\n",
    "\n",
    "# Set data range\n",
    "dates = pd.date_range(start='2018-01-01', periods=36, freq='MS') \n",
    "\n",
    "## Generate exogenous variables\n",
    "\n",
    "# promo_index: cyclical promotional intensity, e.g., seasonal sales campaigns\n",
    "promo_index = 0.5 * np.sin(np.linspace(0, 4 * np.pi, 36)) + 0.1 * np.random.randn(36)\n",
    "\n",
    "# holiday_flag: binary indicator for holiday month (e.g., Nov, Dec as holiday season)\n",
    "holiday_flag = dates.month.isin([11,12]).astype(int)\n",
    "\n",
    "## Generate endogenous variables\n",
    "# Generate base sales with trend\n",
    "base_sales_A = 200 + np.linspace(0, 50, 36)  # increasing trend over months\n",
    "base_sales_B = 300 + np.linspace(0, 30, 36)\n",
    "\n",
    "# Add effects of promo and holidays plus noise\n",
    "sales_A = base_sales_A + 20 * promo_index + 30 * holiday_flag + 15 * np.random.randn(36)\n",
    "sales_B = base_sales_B + 25 * promo_index + 40 * holiday_flag + 20 * np.random.randn(36)\n",
    "\n",
    "## Set table with endogenous and exogenous variables\n",
    "data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'sales_A': sales_A,\n",
    "    'sales_B': sales_B,\n",
    "    'promo_index': promo_index,\n",
    "    'holiday_flag': holiday_flag\n",
    "}).set_index('date')\n",
    "\n",
    "# Create a copy of the original endogenous columns for later verification\n",
    "holdout_endog = data[['sales_A', 'sales_B', 'promo_index', 'holiday_flag']].iloc[-6:].copy()\n",
    "\n",
    "# Erase last 6 months of sales data in the main data DataFrame by setting them to NaN\n",
    "data.loc[data.index[-6:], ['sales_A', 'sales_B']] = np.nan\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a2858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot data\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(data.index, data['sales_A'], marker='o', linestyle='-', color='tab:blue', label='Product A Sales')\n",
    "plt.plot(data.index, data['sales_B'], marker='o', linestyle='-', color='tab:orange', label='Product B Sales')\n",
    "plt.title('Monthly Retail Sales', fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Date\", fontsize=14)\n",
    "plt.ylabel(\"Sales\", fontsize=14)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(data.index, [d.strftime('%Y-%m') for d in data.index], rotation=45, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f385366",
   "metadata": {},
   "source": [
    "**Arrange data as required**  \n",
    "  \n",
    "We'll assume the following:\n",
    "* Available data of endogenous variables are from 2018-01 to 2020-06. Thus, we'll ignore that endogenous data from 2020-07 to 2020-12 is available, but we'll keep it for evaluation porpouse.\n",
    "* Available data of exogenous variables are from 2018-10 to 2020-06. Nevertheless, we'll consider data from 2020-07 to 2020-12 as projected data in order to forecast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30351114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "endog = data[['sales_A', 'sales_B']]\n",
    "exog = data[['promo_index', 'holiday_flag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44a17c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set present data\n",
    "endog_present = endog.iloc[:-6]\n",
    "endog_validation = endog.tail(6)\n",
    "exog_present = exog.iloc[:-6]\n",
    "exog_validation = exog.tail(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad5cab9",
   "metadata": {},
   "source": [
    "## Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d78ef",
   "metadata": {},
   "source": [
    "### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a179eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_df(original_df, I):\n",
    "    \"\"\"\n",
    "    Apply I differences to all columns in the DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    diff_df = original_df.copy()\n",
    "\n",
    "    if I == 0:\n",
    "        return original_df\n",
    "\n",
    "    if I == 1:\n",
    "        for col in diff_df.columns:\n",
    "            diff_df[col] = original_df[col].diff().dropna()\n",
    "            #diff_df[col] = diff_df[col].clip(lower=1e-6)\n",
    "    diff_df = diff_df.dropna()\n",
    "\n",
    "    if I == 2:\n",
    "        for col in diff_df.columns:\n",
    "            diff_df[col] = original_df[col].diff().dropna().diff().dropna()\n",
    "            #diff_df[col] = diff_df[col].clip(lower=1e-6)\n",
    "        diff_df = diff_df.dropna()\n",
    "    \n",
    "    return diff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb8792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_inv_df(original_df, diff_df, I):\n",
    "    \"\"\"\n",
    "    Reverse I differences of a time series to all columns in the DataFrames that has been differenced I times\n",
    "    \"\"\"\n",
    "    if I == 0:\n",
    "        return original_df.copy()\n",
    "\n",
    "\n",
    "    elif I == 1:\n",
    "        rev_df = original_df.copy()\n",
    "        for col in diff_df.columns:\n",
    "            y = pd.Series(original_df[col], index = original_df.index, name=col)\n",
    "            y_0 = y.iloc[0]\n",
    "            y_diff_1 = pd.Series(diff_df[col], index=diff_df.index, name=col)\n",
    "\n",
    "            y_diff_0 = pd.Series(index = y_diff_1.index, dtype='float64', name = col)\n",
    "            cumulative_sum = 0\n",
    "            for timestamp in y_diff_1.index:\n",
    "                cumulative_sum = cumulative_sum + y_diff_1.loc[timestamp]  # same as += but explicit\n",
    "                y_diff_0.loc[timestamp] = y_0 + cumulative_sum\n",
    "            y_diff_0 = pd.concat([pd.Series([y_0], index=[y.index[0]]), y_diff_0])\n",
    "\n",
    "            rev_df[col] = y_diff_0\n",
    "\n",
    "    elif I == 2:\n",
    "        rev_df = original_df.copy()\n",
    "        for col in diff_df.columns:\n",
    "            y = pd.Series(original_df[col], index = original_df.index, name=col)\n",
    "            y_0 = y.iloc[0]\n",
    "            y_1 = y.iloc[1]\n",
    "            y_diff_2 = pd.Series(diff_df[col], index=diff_df.index, name=col)\n",
    "\n",
    "            y_diff_1 = pd.Series(index = y_diff_2.index, dtype='float64', name = col)\n",
    "            cumulative_sum = 0\n",
    "            for timestamp in y_diff_2.index:\n",
    "                cumulative_sum = cumulative_sum + y_diff_2.loc[timestamp]\n",
    "                y_diff_1.loc[timestamp] = (y_1 - y_0) + cumulative_sum\n",
    "            y_diff_1 = pd.concat([pd.Series([y_1 - y_0], index = [y.index[1]]), y_diff_1])\n",
    "            \n",
    "            y_diff_0 = pd.Series(index = y_diff_1.index, dtype='float64', name = col)\n",
    "            cumulative_sum = 0\n",
    "            for timestamp in y_diff_1.index:\n",
    "                cumulative_sum = cumulative_sum + y_diff_1.loc[timestamp]\n",
    "                y_diff_0.loc[timestamp] = y_0 + cumulative_sum\n",
    "            y_diff_0 = pd.concat([pd.Series([y_0], index = [y.index[0]]), y_diff_0])\n",
    "            \n",
    "\n",
    "            rev_df[col] = y_diff_0\n",
    "\n",
    "    return rev_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b57c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "def boxcox_df(df):\n",
    "    \"\"\"\n",
    "    Apply Box-Cox transformation to all columns in the DataFrame\"\n",
    "    \"\"\"\n",
    "    transformed_data = {}\n",
    "    lambdas = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        # Ensure all values are positive\n",
    "        if (df[col] <= 0).any():\n",
    "            raise ValueError(f\"Box-Cox requires positive values. Column '{col}' has non-positive values.\")\n",
    "\n",
    "        bc, l = boxcox(df[col])\n",
    "        transformed_data[col] = bc\n",
    "        lambdas[col] = l\n",
    "\n",
    "    return pd.DataFrame(transformed_data, index=df.index, columns = df.columns), lambdas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b97ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import inv_boxcox\n",
    "\n",
    "def inv_boxcox_df(df_transformed, lambdas):\n",
    "    \"\"\"\n",
    "    Inverse Box-Cox transformation for all columns in the DataFrame\n",
    "    \"\"\"\n",
    "    recovered_data = {}\n",
    "\n",
    "    for col in df_transformed.columns:\n",
    "        recovered = inv_boxcox(df_transformed[col], lambdas[col])\n",
    "        recovered_data[col] = recovered\n",
    "\n",
    "    return pd.DataFrame(recovered_data, index=df_transformed.index, columns = df_transformed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f098b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multivariate_forecast(actual, predicted):\n",
    "    \"\"\"\n",
    "    Computes MAPE, R²,and MAE per time series.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "    MAPE =  []\n",
    "    R2 = []\n",
    "    MAE =  []\n",
    "\n",
    "    for col in actual.columns:\n",
    "        y_true = actual[col]\n",
    "        y_pred = predicted[col]\n",
    "\n",
    "        mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "        MAPE.append(mape)\n",
    "        R2.append(r2)\n",
    "        MAE.append(mae)\n",
    "\n",
    "    metrics = pd.DataFrame({\n",
    "        'MAPE': MAPE,\n",
    "        'R2': R2,\n",
    "        'MAE': MAE\n",
    "    })\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662ccb14",
   "metadata": {},
   "source": [
    "### Grid Seacrh with AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e15c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'AR_p' : [0,1,2],\n",
    "    'MA_q' : [0,1,2],\n",
    "    'I': [0,1,2],\n",
    "    'Box-Cox' : [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f849ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required tools\n",
    "from itertools import product\n",
    "from scipy.stats import boxcox\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "\n",
    "# Hyperparameter grid search function\n",
    "def VARMAX_GRID_SEARCH_AIC(endog, exog, max_p, max_q, bc, freq):\n",
    "    \"\"\"\n",
    "    VARMAX Grid Search Based on AIC\n",
    "\n",
    "    IN:\n",
    "    -> endog: DataFrame with endogenous variables\n",
    "    -> exog: DataFrame with exogenous variables\n",
    "    -> param_grid: dictionary with hyperparameter grid\n",
    "    -> max_p: maximum AR order\n",
    "    -> max_q: maximum MA order\n",
    "    -> freq: frequency of the time series data\n",
    "    \n",
    "    OUT:\n",
    "    -> df_metrics: DataFrame with hyperparameters and AIC\n",
    "    \"\"\"\n",
    "    p = range(0, max_p+1)\n",
    "    q = range(0, max_q+1)\n",
    "    I = [0, 1, 2]\n",
    "    pq = list(product(p, q))\n",
    "\n",
    "    \n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        # Arrange data\n",
    "        for d in I:\n",
    "             \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for param in piq:\n",
    "                try:\n",
    "                    model = SARIMAX(endog = endog_data_bc,\n",
    "                                    exog = exog_data_norm,\n",
    "                                    order=param,\n",
    "                                    seasonal_order=(param_seasonal[0], param_seasonal[1], param_seasonal[2], param_seasonal[3]),\n",
    "                                    )\n",
    "                    results = model.fit()\n",
    "                    if results.aic < best_aic and param != (0,0,0):\n",
    "                        best_aic = results.aic\n",
    "                        best_params = (param, param_seasonal)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "print(f'Best SARIMA model: order={best_params[0]}, seasonal_order={best_params[1]} with AIC={best_aic}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0faea8",
   "metadata": {},
   "source": [
    "### Grid Search with In-Sample Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b236de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'AR_p' : [0,1,2],\n",
    "    'MA_q' : [0,1,2],\n",
    "    'I': [0,1,2],\n",
    "    'Box-Cox' : [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f5f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required tools\n",
    "from itertools import product\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "\n",
    "\n",
    "# Hyperparameter grid search function\n",
    "def VARMAX_GRID_SEARCH_IN_SAMPLE(endog, exog, param_grid, freq):\n",
    "    \"\"\"\n",
    "    VARMAX Hyperparameter Grid Search with In-Sample Evaluation\n",
    "\n",
    "    IN:\n",
    "    -> endog: endogenous variables at present time\n",
    "    -> exog: exogenous variables at present time\n",
    "    -> param_grid: hyperparameter grid to search over\n",
    "    -> freq: frequency of the time series data\n",
    "\n",
    "    OUT:\n",
    "    -> df_metrics: DataFrame with hyperparameters and in-sample metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Print total iterations\n",
    "    print('Total Iterations: ', len(list(product(*param_grid.values()))))\n",
    "\n",
    "    # Initialize metrics df and metrics lists\n",
    "    df_metrics = pd.DataFrame(columns = ['AR_p', 'MA_q', 'I', 'Box-Cox', 'IN_SAMPLE_MAPE', 'IN_SAMPLE_R2', 'IN_SAMPLE_MAE', 'IN_SAMPLE_AVG_MAPE', 'IN_SAMPLE_AVG_R2', 'IN_SAMPLE_AVG_MAE', 'IN_SAMPLE_TIME'])\n",
    "\n",
    "    # Initialize iteration counter \n",
    "    iter = 1\n",
    "\n",
    "    # Perform grid search\n",
    "    for params in product(*param_grid.values()):\n",
    "        print('------------------------------')\n",
    "        print('Iteration: ', iter)\n",
    "        print('Parameters: ', params)\n",
    "\n",
    "        # Discard not suitable parameter combinations\n",
    "        if params[0] == 0 and params[1] == 0:\n",
    "            print('Not suitable parameter combination')\n",
    "        else:\n",
    "\n",
    "            ### Prepare data\n",
    "            # Initialize hyperparameters\n",
    "            p, q, I, bc = params\n",
    "\n",
    "            # Initialize training data\n",
    "            endog_train = endog.copy()\n",
    "            exog_train = exog.copy()\n",
    "\n",
    "            # Handle differences\n",
    "            if I > 0:\n",
    "                endog_train = diff_df(endog_train, I)\n",
    "                exog_train = diff_df(exog_train, I)\n",
    "\n",
    "            # Discard not possible Box-Cox transformation \n",
    "            if bc & (endog_train <= 0).any().any():\n",
    "                print('Box-Cox not suitable because of non-positive values')\n",
    "            else:\n",
    "            \n",
    "                # Handle Box-Cox transformation\n",
    "                if bc:\n",
    "                    endog_train, lambdas = boxcox_df(endog_train)\n",
    "\n",
    "                ### Train model\n",
    "                # Start time\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Set model\n",
    "                model = VARMAX(\n",
    "                    endog = endog_train, \n",
    "                    exog = exog_train, order=(p, q), \n",
    "                    trend='c',\n",
    "                    freq = freq\n",
    "                )\n",
    "\n",
    "                # Train model\n",
    "                results = model.fit(disp=False, freq = freq)\n",
    "\n",
    "                ### Get predicted values and actual values\n",
    "                # Get predicted values\n",
    "                start = endog.index[I]\n",
    "                end = endog.index[-1]\n",
    "                predicted = results.get_prediction(start=start, end=end, dynamic=False)\n",
    "                predicted_mean = predicted.predicted_mean\n",
    "\n",
    "                ## Return predicted values to original scale\n",
    "                # Inverse differences\n",
    "                if I > 0:\n",
    "                    predicted_mean = diff_inv_df(endog, predicted_mean, I)\n",
    "                    predicted_mean = predicted_mean.iloc[I:]\n",
    "                # Inverse Box-Cox\n",
    "                if bc:\n",
    "                    predicted_mean = inv_boxcox_df(predicted_mean, lambdas)\n",
    "\n",
    "                # Set actual values\n",
    "                if I == 0:\n",
    "                    actual = endog\n",
    "                else:\n",
    "                    actual = endog.iloc[I:]\n",
    "                \n",
    "                # Stop time \n",
    "                end_time = time.time()\n",
    "\n",
    "                ### Calculate metrics\n",
    "                metrics = evaluate_multivariate_forecast(actual, predicted_mean)\n",
    "                avg_mape = metrics['MAPE'].mean()\n",
    "                avg_r2 = metrics['R2'].mean()\n",
    "                avg_mae = metrics['MAE'].mean()\n",
    "\n",
    "                # Fill metrics data frame\n",
    "                df_metrics_aux = pd.DataFrame({\n",
    "                    'AR_p': [p],\n",
    "                    'MA_q': [q],\n",
    "                    'I': [I],\n",
    "                    'Box-Cox': [bc],\n",
    "                    'IN_SAMPLE_MAPE': [list(metrics['MAPE'])],\n",
    "                    'IN_SAMPLE_R2': [list(metrics['R2'])],\n",
    "                    'IN_SAMPLE_MAE': [list(metrics['MAE'])],\n",
    "                    'IN_SAMPLE_AVG_MAPE': [avg_mape],\n",
    "                    'IN_SAMPLE_AVG_R2': [avg_r2],\n",
    "                    'IN_SAMPLE_AVG_MAE': [avg_mae],\n",
    "                    'IN_SAMPLE_TIME': [end_time - start_time]\n",
    "                })\n",
    "                print(df_metrics_aux.head())\n",
    "                df_metrics = pd.concat([df_metrics, df_metrics_aux])\n",
    "\n",
    "        # Increase iteration counter\n",
    "        iter = iter + 1 \n",
    "\n",
    "    # Show metrics data frame\n",
    "    df_metrics.head()\n",
    "\n",
    "    # Return hyperparameter-metrics data frame\n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a160850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = VARMAX_GRID_SEARCH_IN_SAMPLE(endog = endog_present, exog = exog_present, param_grid = param_grid, freq = 'MS')\n",
    "df_results.to_csv(r'VARMAX_results_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56f784f",
   "metadata": {},
   "source": [
    "### Train - Test Split Metrics for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7268f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0164592e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3242c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08d56642",
   "metadata": {},
   "source": [
    "## Pendientes\n",
    "* Ahondar en tranformación potencia Box-Cox\n",
    "* Ahondar en número óptimo de diferencias\n",
    "* Evaluar supuestos del modelo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
